{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Black magic (hehehe)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def merge_two_conv(conv1, conv2):\n",
    "    kernel_size_1 = np.array(conv1.weight.size()[-2:])\n",
    "    kernel_size_2 = np.array(conv2.weight.size()[-2:])\n",
    "    kernel_size_merged = kernel_size_1 + kernel_size_2 - 1\n",
    "\n",
    "\n",
    "    in_channels = conv1.weight.size()[1]\n",
    "    out_channels = conv2.weight.size()[0]\n",
    "    inner_channels = conv1.weight.size()[0]\n",
    "\n",
    "    new_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size_merged)\n",
    "    padding = [kernel_size_2[0]-1, kernel_size_2[1]-1]\n",
    "\n",
    "    new_conv.weight.data = torch.conv2d(conv1.weight.data.permute(1, 0, 2, 3),\n",
    "                                        conv2.weight.data.flip(-1, -2),\n",
    "                                        padding=padding).permute(1, 0, 2, 3)\n",
    "\n",
    "\n",
    "    add_x = torch.ones(1, inner_channels, *kernel_size_2)\n",
    "    add_x *= conv1.bias.data[None, :, None, None]\n",
    "\n",
    "    new_conv.bias.data = torch.conv2d(add_x,\n",
    "                                      conv2.weight.data).flatten()\n",
    "\n",
    "    new_conv.bias.data += conv2.bias.data\n",
    "    return new_conv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T18:22:21.941851Z",
     "end_time": "2023-04-12T18:22:21.955213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 3, 4, 5])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = torch.nn.Conv2d(2, 3, 3)\n",
    "conv2 = torch.nn.Conv2d(3, 5, (4, 5))\n",
    "new_conv = merge_two_conv(conv1, conv2)\n",
    "\n",
    "x = torch.randn([1, 2, 9, 9])\n",
    "\n",
    "assert (torch.abs(conv2(conv1(x)) - new_conv(x)) < 1e-6).min()\n",
    "conv1.bias.data[None, :, None, None]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T19:00:22.023708Z",
     "end_time": "2023-04-12T19:00:22.095202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = torchvision.datasets.ImageFolder(\"./characters\", transformations)\n",
    "images_means = torch.empty(size=(len(dataset), 3))\n",
    "images_stds = torch.empty(size=(len(dataset), 3))\n",
    "labels_means = torch.empty(size=(len(dataset), 3))\n",
    "labels_stds = torch.empty(size=(len(dataset), 3))\n",
    "\n",
    "for ind, (image, label) in enumerate(tqdm(dataset)):\n",
    "    images_means[ind] = (torch.mean(image, dim=[1,2]))\n",
    "    images_stds[ind] = (torch.std(image, dim=[1,2]))\n",
    "\n",
    "\n",
    "images_mean = images_means.mean(dim=[0])\n",
    "images_std =  images_stds.mean(dim=[0])\n",
    "\n",
    "\n",
    "print(f\"Images means: {images_mean}\")\n",
    "print(f\"Images std: {images_std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.datasets\n",
    "import torch\n",
    "import PIL\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 128\n",
    "number_of_labels = 42\n",
    "learning_rate = 0.001\n",
    "num_epochs = 150\n",
    "classes = ('abraham_grampa_simpson',\n",
    "            'agnes_skinner',\n",
    "            'apu_nahasapeemapetilon',\n",
    "            'barney_gumble',\n",
    "            'bart_simpson',\n",
    "            'carl_carlson',\n",
    "            'charles_montgomery_burns',\n",
    "            'chief_wiggum',\n",
    "            'cletus_spuckler',\n",
    "            'comic_book_guy',\n",
    "            'disco_stu',\n",
    "            'edna_krabappel',\n",
    "            'fat_tony',\n",
    "            'gil',\n",
    "            'groundskeeper_willie',\n",
    "            'homer_simpson',\n",
    "            'kent_brockman',\n",
    "            'krusty_the_clown',\n",
    "            'lenny_leonard',\n",
    "            'lionel_hutz',\n",
    "            'lisa_simpson',\n",
    "            'maggie_simpson',\n",
    "            'marge_simpson',\n",
    "            'martin_prince',\n",
    "            'mayor_quimby',\n",
    "            'milhouse_van_houten',\n",
    "            'miss_hoover',\n",
    "            'moe_szyslak',\n",
    "            'ned_flanders',\n",
    "            'nelson_muntz',\n",
    "            'otto_mann',\n",
    "            'patty_bouvier',\n",
    "            'principal_skinner',\n",
    "            'professor_john_frink',\n",
    "            'rainier_wolfcastle',\n",
    "            'ralph_wiggum',\n",
    "            'selma_bouvier',\n",
    "            'sideshow_bob',\n",
    "            'sideshow_mel',\n",
    "            'snake_jailbird',\n",
    "            'troy_mcclure',\n",
    "            'waylon_smithers')\n",
    "class_encoder = {}\n",
    "for i in range(len(classes)):\n",
    "    class_encoder[classes[i]]=i\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = os.listdir(img_dir)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.img_dir,self.img_labels[idx])\n",
    "        image = PIL.Image.open(img_path)\n",
    "        label = self.img_labels[idx]\n",
    "        class_indicator = label.rfind('_')\n",
    "        class_str = label[:class_indicator]\n",
    "        label = class_encoder[class_str]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4622, 0.4075, 0.3524],std=[0.2167, 0.1961, 0.2246]),\n",
    "    transforms.Resize((32,32),antialias=True)\n",
    "])\n",
    "\n",
    "\n",
    "full_dataset = torchvision.datasets.ImageFolder(\"./characters\",transformations)\n",
    "train_dataset,valid_dataset = torch.utils.data.random_split(full_dataset,[0.7, 0.3])\n",
    "train_dataset, test_set = torch.utils.data.random_split(full_dataset,[0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=16)\n",
    "print(\"The number of images in a training set is: \", len(train_loader)*batch_size)\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "print(\"The number of images in a test set is: \", len(test_loader)*batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "print(\"The number of images in validation set is: \",len(valid_loader)*batch_size)\n",
    "print(\"The number of batches per epoch is: \", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv6 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.conv8 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv9 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.conv10 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc1 = nn.Linear(512, 42)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        output = self.pool(output)\n",
    "        output = output.view(-1, 512)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=3, verbose=True, threshold=1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "\n",
    "def saveModel():\n",
    "    path = \"./simpsons.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def testAccuracy():\n",
    "    \n",
    "    model.eval()\n",
    "    metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=42).to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            metric(predicted, labels)\n",
    "    f1 = metric.compute()\n",
    "    return f1\n",
    "loss_metric =[]\n",
    "recall_metric=[]\n",
    "accuracy_metric=[]\n",
    "lr_metric=[]\n",
    "\n",
    "def train():\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    comment = f' batch_size = {batch_size} lr = {learning_rate}'\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    for epoch in tqdm.tnrange(num_epochs,position=0,desc=\"Epochs\"):\n",
    "        losses = []\n",
    "        total_correct=0;\n",
    "        total_f1=0;\n",
    "        for _, (images, labels) in enumerate(tqdm.tqdm_notebook(train_loader,position=1,desc=\"Batch iter\",leave=True), 0):\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            total_correct+= get_num_correct(outputs, labels)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        scheduler.step(mean_loss)\n",
    "        print(f\"Loss at epoch {epoch} = {mean_loss}\")\n",
    "        f1 = testAccuracy()\n",
    "        print(f\"For epoch {epoch} F1: {f1}\")\n",
    "        tb.add_scalar(\"Loss\", mean_loss, epoch)\n",
    "        tb.add_scalar(\"Correct\", total_correct, epoch)\n",
    "        tb.add_scalar(\"F1\", f1, epoch)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            saveModel()\n",
    "            best_f1 = f1\n",
    "\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    tb.add_image(\"images\", grid)\n",
    "    tb.add_graph(model, images)\n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def testClassess():\n",
    "    metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=42,average=None).to(device)\n",
    "    with torch.no_grad():\n",
    "        for data in valid_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            metric(predicted, labels)\n",
    "    acc = metric.compute()\n",
    "    for i in range(number_of_labels):\n",
    "        print(f'F1 of {classes[i]} : {acc[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "\n",
    "torchinfo.summary(model, depth=2, input_size=(128, 3, 32,32), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #train()\n",
    "    print('Finished Training')\n",
    "    model = Network().to(device)\n",
    "    path = \"simpsons.pth\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    testClassess()\n",
    "    print(testAccuracy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
